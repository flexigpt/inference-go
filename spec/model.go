package spec

import (
	"time"
)

const (
	DefaultAuthorizationHeaderKey = "Authorization"
	DefaultAPITimeout             = 300 * time.Second

	DefaultAnthropicOrigin                 = "https://api.anthropic.com"
	DefaultAnthropicChatCompletionPrefix   = "/v1/messages"
	DefaultAnthropicAuthorizationHeaderKey = "x-api-key"

	DefaultOpenAIOrigin                = "https://api.openai.com"
	DefaultOpenAIChatCompletionsPrefix = "/v1/chat/completions"

	DefaultFileDataMIME  = "application/octet-stream"
	DefaultImageDataMIME = "image/png"
)

var OpenAIChatCompletionsDefaultHeaders = map[string]string{"content-type": "application/json"}

type (
	ModelName       string
	ReasoningLevel  string
	ReasoningType   string
	ProviderName    string
	ProviderSDKType string
)

const (
	ProviderSDKTypeAnthropic             ProviderSDKType = "providerSDKTypeAnthropicMessages"
	ProviderSDKTypeOpenAIChatCompletions ProviderSDKType = "providerSDKTypeOpenAIChatCompletions"
	ProviderSDKTypeOpenAIResponses       ProviderSDKType = "providerSDKTypeOpenAIResponses"
)

const (
	ReasoningTypeHybridWithTokens ReasoningType = "hybridWithTokens"
	ReasoningTypeSingleWithLevels ReasoningType = "singleWithLevels"
)

const (
	ReasoningLevelNone    ReasoningLevel = "none"
	ReasoningLevelMinimal ReasoningLevel = "minimal"
	ReasoningLevelLow     ReasoningLevel = "low"
	ReasoningLevelMedium  ReasoningLevel = "medium"
	ReasoningLevelHigh    ReasoningLevel = "high"
	ReasoningLevelXHigh   ReasoningLevel = "xhigh"
)

type ReasoningParam struct {
	Type   ReasoningType  `json:"type"`
	Level  ReasoningLevel `json:"level"`
	Tokens int            `json:"tokens"`
}

// Usage captures normalized token accounting across providers.
// If a provider does not expose a particular field, it will be 0.
type Usage struct {
	// Total input tokens for this request (cached + uncached).
	InputTokensTotal int64 `json:"inputTokensTotal"`

	// Input tokens that were served from or written to a cache (if the provider exposes this).
	InputTokensCached int64 `json:"inputTokensCached"`

	// Input tokens that were not cached.
	InputTokensUncached int64 `json:"inputTokensUncached"`

	// Output tokens generated by the model.
	OutputTokens int64 `json:"outputTokens"`

	// Tokens attributed to "reasoning" (if available).
	ReasoningTokens int64 `json:"reasoningTokens"`
}

// ModelParam represents input information about a model to a completion.
type ModelParam struct {
	Name                        ModelName       `json:"name"`
	Stream                      bool            `json:"stream"`
	MaxPromptLength             int             `json:"maxPromptLength"`
	MaxOutputLength             int             `json:"maxOutputLength"`
	Temperature                 *float64        `json:"temperature,omitempty"`
	Reasoning                   *ReasoningParam `json:"reasoning,omitempty"`
	SystemPrompt                string          `json:"systemPrompt"`
	Timeout                     int             `json:"timeout"`
	AdditionalParametersRawJSON *string         `json:"additionalParametersRawJSON"`
}

// ProviderParam represents information about a provider.
type ProviderParam struct {
	Name                     ProviderName      `json:"name"`
	SDKType                  ProviderSDKType   `json:"sdkType"`
	APIKey                   string            `json:"apiKey"`
	Origin                   string            `json:"origin"`
	ChatCompletionPathPrefix string            `json:"chatCompletionPathPrefix"`
	APIKeyHeaderKey          string            `json:"apiKeyHeaderKey"`
	DefaultHeaders           map[string]string `json:"defaultHeaders"`
}
